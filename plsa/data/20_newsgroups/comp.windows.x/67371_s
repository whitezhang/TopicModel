
 
 i am *almost* done porting xfree86 1.2 to a new piece of display
 hardware, but have run into a snag i think may be somewhat
 commonplace, so i'm sending a net-feeler.
 
 i have a display that is a non-interlaced, memory mapped, 1-bit
 720x280 display.  the server's view of the world, (obtained via xwd |
 xwud), seems to be exactly what it should be.  however, the displayed
 version of the framebuffer gives the impression that the server is
 using scanlines that are too long.  after a bit of experimentation, it
 seems that the problem was that the server was padding the line out to
 a word boundry, but the scanline size in the buffer is 90 bytes, which
 isn't exactly divisible by four.  changing the following defines in
 mit/server/include/servermd.h:
 
 ----
 
 #define bitmap_scanline_pad  32
 #define log2_bitmap_pad		5
 #define log2_bytes_per_scanline_pad	2
 
 ---
 
 to:
 
 ---
 
 #define bitmap_scanline_pad  16
 #define log2_bitmap_pad		4
 #define log2_bytes_per_scanline_pad	2
 
 ---
 
 was not exactly the right solution.  how do i tell the server either
 (a) don't pad the scan lines at all ('cause this server is only being
 built to run on this particular display), or to pad only to byte
 boundries?
 
 i'm using a customized version of xfree86v1.2, under mach 3.0.
 
 thanks
 brian
 